<h1>Machine Learning Algorithms</h1>
<h2>Descritive models</h2>
<p>Business Intelligence is an example of this type of model.</p>
<p>The objective is to look to the past and understand what happened.</p>
<p>Example: How much new customers we got last year?</p>
<h2>Predictive Models</h2>
<p>The difference of this type of model to descritive models is the hability to foresee phenomenons.</p>
<p>In other words, the preditive models use data from the past to predict what will happen in the future, while the descritive model use the data from the past to understand what happened in the past.</p>
<p>The predictive model is a approximate mathematical function created with data that can foresee phenomenons.</p>
<h3>Supervisioned Learning</h3>
<p>In this type of learning we give a sample of data and the response for a question, the model learns with that data and try to predict the answer for that same question with new data.</p>
<h3>Non-supervisioned Learning</h3>
<p>In this type of learning, we don't have the output data, and the algorithim try to split the data into categories.</p>
<p>Hard clustering: The output says if a point is from one category or other.</p>
<p>Soft clustering: We calculate the probability of a point be in one category or other.</p>
<h4>Instance based methods</h4>
<p>The idea of the instance based methods is to storage the training examples the generalization is only made when the new instance is materialized.</p>
<p>The idea is calculate the distance between the new instance (for example a request) and the training data. The answer is the same as the nearest point in the training data.</p>
<p>There are some discussions in the academy if it is a model or just an automation, regardless that philosofical issue, the point is that being a model or not, it has the power to solve problems.</p>
<p>Example of models of this type:</p>
<p>KNN - K - Nearest Neighbours</p>
<h5>How the ponderation of distance is made?</h5>
<h6>Continuous data</h6>
<p>We use the euclidian distance (remember of pythagoras theorem, making a triangle.) but the distance Manhattan can be used too when we want to give difference weights to each feature.</p>
<p>To balance the difference between units, we use to normalize the data between [0,1](preprocessing method).</p>
<p>We have other distance metrics to specific situations:</p>
<ul>
    <li><i>Pearson Correlation</i>: Ample use in the bioinformatics and statistics.</li>
    <li><i>Cosine Similarity</i>: Ample use in text classification and high-dimensional data.</li>
    <li><i>Edit Distance</i>: Used to measure the distance between strings, ample use in text classification and bioinformatics.</li>
</ul>
<h6>Discrete data</h6>
<p>If the value is equal, then the distance is 0. If the value is not equal the distance is 1.</p>
<h5>Pros and cons</h5>
<p>A big benefit of this method is that we don't need to know which type of distribution the data has.</p>
<p>A bad thing using this method is that all the processing is made in the time the instance appears, so the processing time can be a bit slowly because to work it needs to compare the new instance with all data of the training dataset.</p>
<h4>Probabilistic methods</h4>
<p>The probabilistic baynesian methods assumes that the probability of two events happens don't depends only on their relation but also in the probability of see those events alone.</p>
<p>Example of models of this type:</p>
<p>Na√Øve Bayes</p>
<h5>Pros</h5>
<p>They are practical machine learning algorithms.</p>
<p>They are famous because of the "golden rule" (smallest error possible) to evaluate other leargnin algorithms.</p>
<h4>Search-based methods</h4>
<p>This type of methods are good to data minning, or in other words: Extraction of informations from data never seen before.</p>
<p>A complex problem is broken into small problems, making easy to find an answer.</p>
<p>Example of models of this type:</p>
<p>Decision Trees</p>
<p>Random Forests</p>
<p>Example of algorithms:</p>
<p>ID3</p>
<p>C4.5</p>
<h5>How the ID3 algorithm work?</h5>
<ol>
    <li>Which atribute is the most important? Let's add it to the root.</li>
    <li>How I know which atribute is the most important? Simple, let's check the entropy.</li>
    <li>What is entropy? It describer the level of disorder in a sample of data. In other words, it show us which atribute is the best to "give order" to the data.</li>
    <li>First the algorithm works with all the examples, selecting the atribute that best order/group the examples, creating an node.</li>
</ol>

<h4>Optimization based methods</h4>
<p>The main objective is to optimize the mathematical function.</p>
<p>The main algorithms of optimization methods:</p>
<p>Neural Artifical Networks: Propagation of data between many perpectrons(neurons).</p>
<p>SVM (Support Vector Machines): It change the dimensions of the data (example: from 2D to 3D) to allow us to draw a line in a way to split data into categories. The biggest difference with Neural Artificial Networks is the existence of Hyperplane (the line that splits the categories).</p>
<p>The main architectures:</p>
<p>Convolutional Neural Networks (CNN): Used mostful in computational vision problems.</p>
<p>Recurrent Neural Networks (RNN): Alows storage info into memory, using those previous results to proccess the next input. An example of use is the suggestion of the next word when someone is typing.</p>
<p>Kohohen network: The perpectron are handled like a collection.</p>
<p>Hopfield network.</p>
<h4>Machine learning with ensemble methods</h4>
<p>It's usually used to win data competitions, achieving high precision.</p>
<p>The idea is to train different models to use their outputs in a complementary way to achieve best precision.</p>
<p>The bad part of using this technique is the high processing time.</p>
<ul>
    <li>Bootstrap Aggregation (Bagging): In this case we use always the same type of algorithim (Example: Random Forest and Bagged CART).</li>

    <li>Bossting: One algorithim error rate is improved with other algorithim.Examples: C5.0, Stochastic Gradient Boosting and AdaBoost.</li>
    <li>Voting: When we use different algorithims and by voting the best for that situation is selected.</li>
</ul>

<h2>Dimensionality Reduction</h2>
<p>It's a data wrangling technique that reduces the dimensionality of a problem, or in other words, some business problems have so much features that becomes impossible to make a model, using this method we can "simplify" the number of dimensions (features/columns).</p>
<h3>Feature Extraction: I choose which features I must mantain.</h3>
<p>Examples: Principal Component Analysis (PCA), Multidimensional Scaling and FastMap.</p>
<h3>Feature Selection: I choose which features I must mantain or.I choose which features I can remove.</h3>
<p>Examples: Random Forest, Fractal Dimension Calculus, Wrapper.</p>
<h3>Dimensionality Reduction techniques</h3>
<ul>
    <li><u>Missing Values Ratio</u>: I remove Features with biggest missing values.</li>
    <li><u>Low Variance Filter</u>: I remove features with low variance, because if it has low variance so it will have low power to discriminate behaviours.</li>
    <li><u>High Correlation Filter</u>: I remove features with high correlation, or in other words, same kind of information.</li>
    <li><u>Random Forests/Ensemble Trees</u>: An model in an aumatized way select the best features for me, and I use the best features to create the model I want.</li>
    <li><u>Forward Feature Construction</u>: I will adding each feature by time to check how is the model metrics until find the best combination.</li>
    <li><u>Backward Feature Elimination</u>: I create the model with all features, and I will removing feature by feature to follow how is the model behavior until find the best combination.</li>
    <li><u>Principal Component Analysis (PCA)</u>: Non supervisioned algorithim method that help us to find the principal component in a sample of features. We bacisally turns X features into two principal components, representing the information of those X features. <b>To use this method the data MUST BE SCALED!</b></li>
</ul>